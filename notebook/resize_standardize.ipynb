{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resize_standardize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct4ONRRWX9L9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNZ-uW6VTDpY"
      },
      "source": [
        "import wave\n",
        "import os\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AvTP8ecTFFP"
      },
      "source": [
        "Path_to_train = \"train\\wav\"\n",
        "subfolders = os.listdir(Path_to_train)\n",
        "data = []\n",
        "for s in subfolders:\n",
        "    files = os.listdir(Path_to_train + \"/\" +s)\n",
        "    data.extend([Path_to_train + \"/\" + s+ \"/\" + f for f in files])\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeZyvnnJTFHf"
      },
      "source": [
        "#read text from every transcription audio\n",
        "def read_text( text_path):\n",
        "    text = []\n",
        "    with open(text_path) as fp:\n",
        "        line = fp.readline()\n",
        "        while line:\n",
        "        # TODO: fix spaces in in amharic text\n",
        "            text.append(line)\n",
        "            line = fp.readline()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykuyj212TFK-"
      },
      "source": [
        "#extract the transcription and the label \n",
        "label=[]\n",
        "transcriptions = []\n",
        "for t in text:\n",
        "    sp = t.split(\"\\t\")\n",
        "    sp = sp.strip(\"\\n\")\n",
        "    if len(sp) > 1:\n",
        "        label.append(sp[0])\n",
        "        transcriptions.append(sp[1])\n",
        "transcriptions[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8mqLRCHTY8L"
      },
      "source": [
        "#get audio path , every path must corespond to transcription , get the transprion in the doc and append to audio path \n",
        "audio_path=[0]*len(transcriptions)\n",
        "for d in data:\n",
        "    _d = d.strip(\".wav\")\n",
        "    sp = _d.split(\"/\")[2]\n",
        "    index = label.index(sp)\n",
        "    audio_path[index] = d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atBdLgb2TY_i"
      },
      "source": [
        "#calculate duration \n",
        "duration_of_recordings=[]\n",
        "for d in audio_path:\n",
        "    audio, fs = librosa.load(d, sr=None)\n",
        "    duration_of_recordings.append(float(len(audio)/fs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuUupPpyTZCP"
      },
      "source": [
        "import pandas as pd \n",
        "data=pd.DataFrame({'key': audio_path,'text': transcriptions, 'duration':duration_of_recordings})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfqPBqisTsd0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mNnSAu0rvhl"
      },
      "source": [
        "# Resize and standardize\n",
        "We resize all the audio samples to have the same length by either extending its duration by padding it with silence, or by truncating it.\n",
        "We use the right padding method\n",
        "\n",
        "Standardize the sample rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNbZwxa3TFOX"
      },
      "source": [
        "# used to load audio file\n",
        "#specifying sample rate will resize all the files i.e Audio will be automatically resampled to the given rate\n",
        "class Loader:\n",
        "  def __init__(self, sample_rate,duration,mono):\n",
        "    self.sample_rate=sample_rate\n",
        "    self.duration=duration\n",
        "    self.mono=mono\n",
        "    self.channel = 2\n",
        "\n",
        "\n",
        "  def load(self,filepath):\n",
        "    sig, sr = torchaudio.load(filepath)\n",
        "    aud = sig, sr\n",
        "    return aud\n",
        "\n",
        "  #before using this function kindly change your file paths for it to work\n",
        "\n",
        "\n",
        "  def rechannel(self, aud):    #convert mono to stereo\n",
        "    # aud=self.aud\n",
        "    sig, sr = aud\n",
        "  \n",
        "\n",
        "    if (sig.shape[0] == self.channel):\n",
        "      # Nothing to do\n",
        "      return self.aud\n",
        "\n",
        "    if (self.channel == 1):\n",
        "      # Convert from stereo to mono by selecting only the first channel\n",
        "      resig = sig[:1, :]\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      resig = torch.cat([sig, sig])\n",
        "\n",
        "    aud = resig, sr\n",
        "  def resample(self,aud):                    #standardize sample rate\n",
        "    sig, sr = aud\n",
        "    \n",
        "    if (sr == self.sample_rate):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    num_channels = sig.shape[0]\n",
        "    # Resample first channel\n",
        "    resig = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[:1,:])\n",
        "    if (num_channels > 1):\n",
        "      # Resample the second channel and merge both channels\n",
        "      retwo = torchaudio.transforms.Resample(sr, self.sample_rate)(sig[1:,:])\n",
        "      resig = torch.cat([resig, retwo])\n",
        "      aud = resig, self.sample_rate\n",
        "    return aud\n",
        "\n",
        "  # ----------------------------\n",
        "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "  # ----------------------------\n",
        "  def pad_trunc(self,aud):\n",
        "    sig, sr = aud\n",
        "    num_rows, sig_len = sig.shape\n",
        "    max_len = sr//1000 * self.duration\n",
        "\n",
        "    if (sig_len > max_len):\n",
        "      # Truncate the signal to the given length\n",
        "      sig = sig[:,:max_len]\n",
        "\n",
        "    elif (sig_len < max_len):\n",
        "      # Length of padding to add at the beginning and end of the signal\n",
        "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "      pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "      # Pad with 0s\n",
        "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "      aud = sig, sr\n",
        "    return aud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkLUE2j5TFS0"
      },
      "source": [
        "class PreprocessingPipeline:\n",
        "  '''Processes audio files in a directory by applying the following steps\n",
        "    1. Load the data, convert to stereo and resample sampling rate\n",
        "    2. Pad the audio\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.padder=None\n",
        "    self._loader=None\n",
        "   \n",
        "\n",
        "  def process(self,audio_files_directory):\n",
        "    for root, directories, files in os.walk(audio_files_directory):\n",
        "        for filename in files:\n",
        "            filepath = os.path.join(root, filename)\n",
        "            self._process_file(filepath)\n",
        "            print(f\"Processed file {filepath}\")\n",
        "    \n",
        "  def _process_file(self,filepath):\n",
        "    signal=self.loader.load(filepath)\n",
        "    signal = self.loader.make_stereo(signal)\n",
        "    signal = self.loader.resample(signal)\n",
        "    signal= self.loader.pad_trunc(signal)\n",
        "    \n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeWtzfg9TFVS"
      },
      "source": [
        "DURATION=4000\n",
        "SAMPLE_RATE=44100\n",
        "MONO=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObprKDYvUPOb"
      },
      "source": [
        "loader=Loader(SAMPLE_RATE, DURATION, MONO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBtGze_5UPRG"
      },
      "source": [
        "preprocessing_pipeline=PreprocessingPipeline()\n",
        "preprocessing_pipeline.loader=loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf0oX_sfUPT-"
      },
      "source": [
        "preprocessing_pipeline.process(Path_to_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef5TTLS0UPm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}